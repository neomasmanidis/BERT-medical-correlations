{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-02T00:46:11.402781Z",
     "iopub.status.busy": "2023-03-02T00:46:11.402070Z",
     "iopub.status.idle": "2023-03-02T00:46:11.412390Z",
     "shell.execute_reply": "2023-03-02T00:46:11.411319Z",
     "shell.execute_reply.started": "2023-03-02T00:46:11.402719Z"
    },
    "papermill": {
     "duration": 4.354008,
     "end_time": "2023-02-22T00:54:47.948285",
     "exception": false,
     "start_time": "2023-02-22T00:54:43.594277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, median_absolute_error, mean_absolute_error,mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_validate, cross_val_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers.models.bert import modeling_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used in all training notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T00:46:11.446310Z",
     "iopub.status.busy": "2023-03-02T00:46:11.445914Z",
     "iopub.status.idle": "2023-03-02T00:46:11.490985Z",
     "shell.execute_reply": "2023-03-02T00:46:11.489768Z",
     "shell.execute_reply.started": "2023-03-02T00:46:11.446279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to map hexbin plot colors\n",
    "def my_reduce(x):\n",
    "    npsum = np.sum(x)\n",
    "    return np.log(npsum)\n",
    "\n",
    "\n",
    "# Function to train model (1 epoch)\n",
    "def train_model(model,input_function, train_loader, criterion, optimizer, verbose = False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.train() # enable train mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    if verbose == 2:\n",
    "        status = train_loader\n",
    "    else:\n",
    "        status = tqdm_notebook(train_loader, 0)\n",
    "        \n",
    "    for i, data in enumerate(status):\n",
    "        # get the inputs; data is a tuple of lists [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        input_tensors = input_function(inputs)\n",
    "        labels =  labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(*input_tensors)\n",
    "        loss = criterion(torch.squeeze(outputs).to(torch.float32),labels.to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        if verbose !=2:\n",
    "            running_loss += loss.item()\n",
    "            if i % 500 == 499:    # print every 500 batches\n",
    "                tqdm.write(f'Batch {i + 1:5d}    Loss: {running_loss / 500:.5f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "                \n",
    "# Function to test model            \n",
    "def test_model(model,input_function, test_loader, criterion, verbose = False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.eval() # enable evaluation mode\n",
    "    valid_loss = 0\n",
    "    valid_preds, valid_labels = [], []\n",
    "    \n",
    "    if verbose == 2:\n",
    "        status = test_loader\n",
    "    else:\n",
    "        status = tqdm_notebook(test_loader, 0)\n",
    "        \n",
    "    for i, data in enumerate(status):\n",
    "        # get the inputs; data is a tuple of lists: [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        input_tensors = input_function(inputs)\n",
    "        labels =  labels.to(device)\n",
    "        \n",
    "        outputs = model(*input_tensors)\n",
    "        loss = criterion(outputs[0].to(torch.float32),labels.to(torch.float32))\n",
    "        valid_preds.append(outputs[0].cpu().detach().item())\n",
    "        valid_labels.append(labels.cpu().detach().item())\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    valid_mse = mean_squared_error(valid_labels, valid_preds)\n",
    "    valid_maerr = mean_absolute_error(valid_labels, valid_preds)\n",
    "    \n",
    "    if verbose !=2:\n",
    "        print(\"\\nTest Results:\\nLoss: {:.6f}\".format(valid_loss / len(test_loader)))\n",
    "        print(\"Total Mean Squared Error: {:.4f}\".format(valid_mse))\n",
    "        print(\"Total Mean Absolute Error: {:.4f}\\n\".format(valid_maerr))\n",
    "\n",
    "        # Plots\n",
    "\n",
    "        # Prepare Hexbin plot\n",
    "        temp_df = pd.DataFrame(zip(valid_preds,valid_labels), columns = ['Predicted values', 'Hops'])\n",
    "#         temp_df['Hops'] = 1/temp_df['Hops']\n",
    "        temp_df['C'] = 1\n",
    "\n",
    "        if verbose:\n",
    "            fig, axes = plt.subplots(4,1, figsize = (10,20))\n",
    "        else:\n",
    "            fig, axes = plt.subplots(2,1, figsize = (10,10))\n",
    "\n",
    "        temp_df.plot.hexbin(ax = axes[0], x ='Hops', y='Predicted values',C = 'C', reduce_C_function=my_reduce,colormap=\"viridis\")\n",
    "        axes[0].set_title(\"Hexbin Plot\")\n",
    "    \n",
    "    # Calculate metrics per Hop\n",
    "    mse = []\n",
    "    mae = []\n",
    "    high1_p=high_close_p=high_mid_p=high_far_p=high1_n=high_close_n=high_mid_n=high_far_n=0\n",
    "    for i in range(1,31):\n",
    "        target = i\n",
    "#         target = i**2\n",
    "        preds = []\n",
    "        trues = []\n",
    "        for predicted, true in zip(valid_preds, valid_labels):\n",
    "            if true == target:\n",
    "                preds.append(predicted)\n",
    "                trues.append(true)\n",
    "                if predicted < 5:\n",
    "                    if i==1:\n",
    "                        high1_p +=1\n",
    "                    elif i<4:\n",
    "                        high_close_p +=1\n",
    "                    elif i<11:\n",
    "                        high_mid_p +=1\n",
    "                    else:\n",
    "                        high_far_p +=1\n",
    "                else:\n",
    "                    if i==1:\n",
    "                        high1_n +=1\n",
    "                    elif i<4:\n",
    "                        high_close_n +=1\n",
    "                    elif i<11:\n",
    "                        high_mid_n +=1\n",
    "                    else:\n",
    "                        high_far_n +=1\n",
    "        if len(trues)==0:\n",
    "            continue\n",
    "        mae.append(mean_absolute_error(trues, preds))\n",
    "        mse.append(mean_squared_error(trues, preds))\n",
    "        if i<4 and verbose != 2:\n",
    "            print(\" Hop '\"+str(i)+\"'\\tMAE: {:.4f}, MSE: {:.4f}\".format(mae[i-1],mse[i-1])) \n",
    "        if i==1:\n",
    "            hist_preds = preds\n",
    "    if high1_n == 0: high1_n=1\n",
    "    if high_close_n == 0: high_close_n=1\n",
    "    if high_mid_n == 0: high_mid_n=1\n",
    "    if high_far_n == 0: high_far_n=1\n",
    "    p1 = high1_p*100/(high1_p+high1_n)\n",
    "    p2 = high_close_p*100/(high_close_p+high_close_n)\n",
    "    p3 = high_mid_p*100/(high_mid_p+high_mid_n)\n",
    "    p4 = high_far_p*100/(high_far_p+high_far_n)\n",
    "    tp = high1_p\n",
    "    tn = high_mid_n + high_far_n\n",
    "    fp = high_mid_p + high_far_p\n",
    "    fn = high1_n\n",
    "    pseudo_precision = tp/(tp + fp)\n",
    "    pseudo_recall = tp/(tp + fn)\n",
    "    pseudo_accuracy = (tp + tn) / (tp + fn + tn +fp)\n",
    "    pseudo_f1 = 2 * pseudo_precision * pseudo_recall / (pseudo_precision + pseudo_recall)\n",
    "    proportion = (high1_n + high1_p)/(high_mid_p + high_mid_n + high_far_p + high_far_n)\n",
    "#     precisionGain = (pseudo_precision - proportion)/(pseudo_precision * (1 - proportion))\n",
    "#     recallGain = (pseudo_recall - proportion)/(pseudo_recall * (1 - proportion))\n",
    "    f1g = (pseudo_f1 - proportion)/(pseudo_f1 * (1 - proportion))\n",
    "    \n",
    "    if verbose !=2:\n",
    "#         print(\"\\nPercentage of high prediction (>0.5): \")\n",
    "        print(\"\\nPercentage of prediction < 5: \")\n",
    "        print(\" Hop  1: \\t{:.4f}%\".format(p1))\n",
    "        print(\" Hops 2-3: \\t{:.4f}%\".format(p2))\n",
    "        print(\" Hops 4-10: \\t{:.4f}%\".format(p3))\n",
    "        print(\" Hops 11+: \\t{:.4f}%\".format(p4))\n",
    "        \n",
    "        print(\"\\nPseudo binary metrics (ignoring Hops 2 & 3): \")\n",
    "        print(\"Precision: \\t{:.4f}%\".format(pseudo_precision*100))\n",
    "        print(\"Recall: \\t{:.4f}%\".format(pseudo_recall*100))\n",
    "        print(\"Accuracy: \\t{:.4f}%\".format(pseudo_accuracy*100))\n",
    "        print(\"f1: \\t{:.4f}%\".format(pseudo_f1*100))\n",
    "        print(\"f1 Gain: \\t{:.4f}%\".format(f1g*100))\n",
    "    \n",
    "        if high1_n>1:\n",
    "#             sns.histplot(hist_preds,ax = axes[1], bins = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3], color = 'lime')\n",
    "            sns.histplot(hist_preds,ax = axes[1], color = 'lime')\n",
    "        axes[1].set_title(\"Prediction distribution for Hop '1'\")\n",
    "        axes[1].grid(axis = 'y', alpha = 0.5)\n",
    "\n",
    "        if verbose:\n",
    "            axes[2].bar(range(1,len(mse)+1), mse, color = 'lightcoral')\n",
    "            axes[2].set_title(\"Mean Squared Error per Hop\")\n",
    "            axes[2].grid(axis = 'y', alpha = 0.5)\n",
    "            for i, v in enumerate(mse):\n",
    "                axes[2].text(i+0.9 , 0.01, str(round(v,4)), color='black',fontsize = 8, fontweight='bold', rotation = 90)\n",
    "\n",
    "            axes[3].bar(range(1,len(mae)+1), mae, color = 'royalblue')\n",
    "            axes[3].set_title(\"Mean Absolute Error per Hop\")\n",
    "            axes[3].grid(axis = 'y', alpha = 0.5)\n",
    "            for i, v in enumerate(mae):\n",
    "                axes[3].text(i+0.9 , 0.01, str(round(v,4)), color='black',fontsize = 8, fontweight='bold', rotation = 90)\n",
    "        plt.show()\n",
    "   \n",
    "    return {'pseudo_precision': pseudo_precision, 'pseudo_recall':pseudo_recall, 'pseudo_accuracy': pseudo_accuracy, 'pseudo_f1': pseudo_f1 ,'mse_list': mse, 'mae_list': mae, \"Hop1 High\": p1,'Hop2-3 High%': p2, 'Hop4-10 High%': p3, 'Hop11+ High%': p4, 'loss':(valid_loss / len(test_loader))}\n",
    "\n",
    "\n",
    "def save_for_best_mse1(metrics_dict):\n",
    "    return metrics_dict['mse_list'][0]\n",
    "\n",
    "# Function to train model and test model for multiple epochs\n",
    "def AIO(model, input_function, train_loader, valid_loader, EPOCHS, criterion, optimizer,save_name, save_criterion_func = save_for_best_mse1, train_loader2 = None, verbose = False):\n",
    "    try:\n",
    "        os.mkdir('./'+save_name+'/') \n",
    "    except:\n",
    "        if verbose != 2:\n",
    "            print(\"Couldn't make new directory \"+save_name+ \", it already exists?\")\n",
    "    \n",
    "    # Initialize best save criterion value\n",
    "    best_save_criterion = 100000000\n",
    "    last_epoch = 0\n",
    "    \n",
    "    # Use GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Start training\n",
    "    for epoch in range(1, EPOCHS+1):  # loop over the dataset multiple times\n",
    "        if verbose != 2: tqdm.write(\"Epoch \"+str(epoch))\n",
    "            \n",
    "        # Train\n",
    "        train_model(model,input_function, train_loader, criterion, optimizer, verbose)\n",
    "        \n",
    "        # If subsampled train set is provided, test to monitor overfitting behaviour\n",
    "        if train_loader2 is not None:\n",
    "            test_model(model, input_function, train_loader2, criterion, verbose)\n",
    "            \n",
    "        # Test on validation dataset   \n",
    "        valid_metrics = test_model(model, input_function, valid_loader, criterion, verbose)\n",
    "        \n",
    "        # If validation performance is better, save model\n",
    "        save_criterion = save_criterion_func(valid_metrics)\n",
    "        if save_criterion < best_save_criterion:\n",
    "            best_save_criterion = save_criterion\n",
    "            last_epoch = epoch\n",
    "            torch.save(model, './'+save_name+'/Model.pth')\n",
    "            # If the model uses a tokenizer as input, save it\n",
    "            try:\n",
    "                input_function.save_pretrained('./'+save_name+'/custom_tokenizer/') \n",
    "            except:\n",
    "                pass\n",
    "            if verbose != 2:\n",
    "                print(\"New best Validation! Saving weights...\")\n",
    "                \n",
    "    if verbose == 2:\n",
    "        return valid_metrics['loss'], last_epoch\n",
    "    print('Finished Training')\n",
    "    return valid_metrics['loss'], last_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T00:46:11.654434Z",
     "iopub.status.busy": "2023-03-02T00:46:11.654013Z",
     "iopub.status.idle": "2023-03-02T00:46:11.677187Z",
     "shell.execute_reply": "2023-03-02T00:46:11.676365Z",
     "shell.execute_reply.started": "2023-03-02T00:46:11.654397Z"
    },
    "papermill": {
     "duration": 21.296037,
     "end_time": "2023-02-22T00:55:09.247790",
     "exception": false,
     "start_time": "2023-02-22T00:54:47.951753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inverse_transform(Hops):\n",
    "    return (1/Hops)\n",
    "\n",
    "\n",
    "def joining(text1, text2):\n",
    "    text = text1\n",
    "    text += f\" [SEP] \"+text2\n",
    "    return text\n",
    "\n",
    "\n",
    "def Load_Datasets(Dataset_version,y_transform = inverse_transform, Max_Hops_Samples_relativeToHop1 = 1.1, ValidationPercentageSplit = 0.1, SubTrainPercentage = 0.1, verbose = True):\n",
    "    # Load whole Train dataset\n",
    "    data_train = pd.read_csv('../KEGG Undirectred Graph Dataset/Hops Dataset '+Dataset_version+' - A.csv', header=0)\n",
    "    data_train = data_train[data_train[\"Hops\"]>0]\n",
    "\n",
    "    # Load whole Test dataset\n",
    "    data_test = pd.read_csv('../KEGG Undirectred Graph Dataset/Hops Dataset '+Dataset_version+' - B.csv', header=0)\n",
    "    data_test = data_test[data_test[\"Hops\"]>0]\n",
    "\n",
    "    # Balanced (subsampled) datasets\n",
    "    data_train_balanced = data_train.groupby('Hops', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), round(len(data_train[data_train['Hops']==1])*Max_Hops_Samples_relativeToHop1))))\n",
    "\n",
    "    data_test_balanced = data_test.groupby('Hops', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), round(len(data_test[data_test['Hops']==1])*Max_Hops_Samples_relativeToHop1))))\n",
    "\n",
    "    # Validation Dataset. Initialize from data\n",
    "    data_valid = data_test.groupby('Hops', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), round(len(data_test_balanced[data_test_balanced['Hops']==1])*ValidationPercentageSplit))))\n",
    "    # Remove any entry given to the validation set from the balanced test set\n",
    "    data_test = data_test[~data_test.isin(data_valid)].dropna()\n",
    "\n",
    "    # Subsampled train set to monitor overfitting behaviour\n",
    "    data_sub_train = data_train_balanced.groupby('Hops', group_keys=False).apply(\n",
    "        lambda x: x.sample(min(len(x), round(len(data_train_balanced[data_train_balanced['Hops']==1])*SubTrainPercentage))))\n",
    "\n",
    "    if verbose: \n",
    "        fig, axes = plt.subplots(1,2, figsize = (25,10), sharey = True, sharex = True)\n",
    "        all_sns_bins = list(range(1,25))\n",
    "        print(\"Raw Train data: \"+str(data_train.shape[0])+\" samples\")\n",
    "        sns.histplot(data_train[\"Hops\"],ax = axes[0], bins = all_sns_bins)\n",
    "        axes[0].set_title(\"Raw Train data: \"+str(data_train.shape[0])+\" samples\")\n",
    "        print(\"Raw Test data: \"+str(data_test.shape[0])+\" samples\")\n",
    "        sns.histplot(data_test[\"Hops\"],ax = axes[1], bins = all_sns_bins)\n",
    "        axes[1].set_title(\"Raw Test data: \"+str(data_test.shape[0])+\" samples\")\n",
    "        plt.show()\n",
    "\n",
    "        fig, axes = plt.subplots(1,3, figsize = (25,10), sharey = True, sharex = True)\n",
    "        print(\"Balanced Train data: \"+str(data_train_balanced.shape[0])+\" samples\")\n",
    "        sns.histplot(data_train_balanced[\"Hops\"],ax = axes[0], bins = all_sns_bins)\n",
    "        axes[0].set_title(\"Balanced Train data: \"+str(data_train_balanced.shape[0])+\" samples\")\n",
    "        print(\"Validation data: \"+str(data_valid.shape[0])+\" samples\")\n",
    "        sns.histplot(data_valid[\"Hops\"],ax = axes[1], bins = all_sns_bins)\n",
    "        axes[1].set_title(\"Validation data: \"+str(data_valid.shape[0])+\" samples\")\n",
    "        sns.histplot(data_test_balanced[\"Hops\"],ax = axes[2], bins = all_sns_bins)\n",
    "        axes[2].set_title(\"Balanced Test data - no overlap: \"+str(data_test_balanced.shape[0])+\" samples\")\n",
    "        plt.show()\n",
    "\n",
    "    data_train_balanced[\"Hops\"] = y_transform(data_train_balanced[\"Hops\"])\n",
    "    if verbose: print(data_train_balanced)\n",
    "    y_train = pd.to_numeric(data_train_balanced.Hops)#.astype(int)\n",
    "    x_train = np.vectorize(joining)(data_train_balanced.Head.astype(str),data_train_balanced.Tail.astype(str))\n",
    "    if verbose: print(x_train[0])\n",
    "        \n",
    "    data_valid[\"Hops\"] = y_transform(data_valid[\"Hops\"])\n",
    "    y_valid = pd.to_numeric(data_valid.Hops)#.astype(int)\n",
    "    x_valid = np.vectorize(joining)(data_valid.Head.astype(str),data_valid.Tail.astype(str))\n",
    "\n",
    "    data_sub_train[\"Hops\"] = y_transform(data_sub_train[\"Hops\"])\n",
    "    y_sub_train = pd.to_numeric(data_sub_train.Hops)#.astype(int)\n",
    "    x_sub_train = np.vectorize(joining)(data_sub_train.Head.astype(str),data_sub_train.Tail.astype(str))\n",
    "\n",
    "    data_test_balanced[\"Hops\"] = y_transform(data_test_balanced[\"Hops\"])\n",
    "    y_test_balanced = pd.to_numeric(data_test_balanced.Hops)#.astype(int)\n",
    "    x_test_balanced = np.vectorize(joining)(data_test_balanced.Head.astype(str),data_test_balanced.Tail.astype(str))\n",
    "    \n",
    "    return x_train,y_train,x_valid,y_valid,x_sub_train,y_sub_train,x_test_balanced,y_test_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T00:46:11.699060Z",
     "iopub.status.busy": "2023-03-02T00:46:11.698624Z",
     "iopub.status.idle": "2023-03-02T00:46:25.302333Z",
     "shell.execute_reply": "2023-03-02T00:46:25.301154Z",
     "shell.execute_reply.started": "2023-03-02T00:46:11.699023Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../KEGG Undirectred Graph Dataset/Hops Dataset v8 - A.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7352/2155303339.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# y_transform = inverse_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0my_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mno_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m x_train,y_train,x_valid,y_valid,x_sub_train,y_sub_train,x_test_balanced,y_test_balanced = Load_Datasets(\n\u001b[0m\u001b[0;32m     10\u001b[0m      'v8',y_transform = y_transform, Max_Hops_Samples_relativeToHop1 = 1.1, ValidationPercentageSplit = 0.05, SubTrainPercentage = 0.05)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7352/2842574823.py\u001b[0m in \u001b[0;36mLoad_Datasets\u001b[1;34m(Dataset_version, y_transform, Max_Hops_Samples_relativeToHop1, ValidationPercentageSplit, SubTrainPercentage, verbose)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mLoad_Datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset_version\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMax_Hops_Samples_relativeToHop1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValidationPercentageSplit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSubTrainPercentage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Load whole Train dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../KEGG Undirectred Graph Dataset/Hops Dataset '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mDataset_version\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' - A.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Hops\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../KEGG Undirectred Graph Dataset/Hops Dataset v8 - A.csv'"
     ]
    }
   ],
   "source": [
    "def squared(Hops):\n",
    "    return Hops**2\n",
    "\n",
    "def no_transform(Hops):\n",
    "    return Hops\n",
    "\n",
    "# y_transform = inverse_transform\n",
    "y_transform = no_transform\n",
    "x_train,y_train,x_valid,y_valid,x_sub_train,y_sub_train,x_test_balanced,y_test_balanced = Load_Datasets(\n",
    "     'v8',y_transform = y_transform, Max_Hops_Samples_relativeToHop1 = 1.1, ValidationPercentageSplit = 0.05, SubTrainPercentage = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T00:46:25.305151Z",
     "iopub.status.busy": "2023-03-02T00:46:25.304755Z",
     "iopub.status.idle": "2023-03-02T00:46:25.341138Z",
     "shell.execute_reply": "2023-03-02T00:46:25.339971Z",
     "shell.execute_reply.started": "2023-03-02T00:46:25.305117Z"
    },
    "papermill": {
     "duration": 0.014075,
     "end_time": "2023-02-22T00:55:09.313214",
     "exception": false,
     "start_time": "2023-02-22T00:55:09.299139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2*768, 512), nn.ReLU(), nn.Dropout(0.1),\n",
    "#             nn.Linear(2048, 2048), nn.ReLU(), nn.Dropout(0.4),\n",
    "#             nn.Linear(2048, 1024), nn.ReLU(), nn.Dropout(0.2),\n",
    "#             nn.Linear(1024,512), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(512, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, *inputs, **args):\n",
    "        logits = self.linear_relu_stack(torch.stack(list(inputs), dim=0))\n",
    "        return logits\n",
    "\n",
    "# Load Embeddings Dictionary  \n",
    "import pickle\n",
    "with open('../Feature Extraction/node_embeddings_dict.pkl', 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)\n",
    "    \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Function to prepare input data for above model\n",
    "def FE_inputs(inputs_list):\n",
    "    input_embeddings = []\n",
    "    for row in inputs_list:\n",
    "        X_ls = row.split(' [SEP] ')\n",
    "        input_embeddings.append(np.append(embeddings_dict[X_ls[0]],embeddings_dict[X_ls[1]]))\n",
    "    return torch.FloatTensor(np.array(input_embeddings)).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T00:46:25.343058Z",
     "iopub.status.busy": "2023-03-02T00:46:25.342681Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "# Custom loss function\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean(((output - target)**2)) # MSE\n",
    "#     loss = torch.mean(((output - target)**2)*target) # More attention to low hops\n",
    "    return loss\n",
    "\n",
    "def save_for_best_loss(metrics):\n",
    "    return metrics[\"loss\"]\n",
    "\n",
    "# Hyper parameters\n",
    "epochs = 10\n",
    "LR = 2e-5\n",
    "BSZ = 32\n",
    "criterion = MeanAbsolutePercentageError().to('cpu')\n",
    "# criterion = my_loss\n",
    "model = NeuralNetwork()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# print(model)\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = DataLoader(tuple(zip(x_train,y_train)), batch_size=BSZ, shuffle=True)\n",
    "sub_train_loader = DataLoader(tuple(zip(x_sub_train,y_sub_train)), batch_size=1, shuffle=True)\n",
    "valid_loader = DataLoader(tuple(zip(x_valid,y_valid)), batch_size=1, shuffle=True)\n",
    "\n",
    "# Train and Test(sub-test)\n",
    "_,stopping_epoch = AIO(model, FE_inputs, train_loader, valid_loader, epochs, criterion, optimizer,'PubMedBERT', save_criterion_func = save_for_best_loss, train_loader2 = sub_train_loader)\n",
    "print(stopping_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 41.02986,
     "end_time": "2023-02-22T00:59:00.891503",
     "exception": false,
     "start_time": "2023-02-22T00:58:19.861643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loader for final testing (balanced)\n",
    "test_balanced_loader = DataLoader(tuple(zip(x_test_balanced,y_test_balanced)), batch_size=1, shuffle=True)\n",
    "\n",
    "# Load Best version for testing\n",
    "model_PMB = torch.load(\"./PubMedBERT/Model.pth\")#, map_location=torch.device('cpu'))\n",
    "model_PMB.eval()\n",
    "print(\"Test trained model on balanced test set - no common nodes with train\")\n",
    "balanced_test_metrics = test_model(model_PMB, FE_inputs, test_balanced_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 38.820664,
     "end_time": "2023-02-22T00:59:39.738996",
     "exception": false,
     "start_time": "2023-02-22T00:59:00.918332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Test model on all original train data\")\n",
    "train_loader_for_testing = DataLoader(tuple(zip(x_train,y_train)), batch_size=1, shuffle=True)\n",
    "balanced_train_metrics = test_model(model_PMB, FE_inputs, train_loader_for_testing, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 80.483281,
     "end_time": "2023-02-22T01:01:00.251541",
     "exception": false,
     "start_time": "2023-02-22T00:59:39.768260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Test model on all distant pairs\") \n",
    "data_dis = pd.read_csv('/kaggle/input/biobert-tutorial/Distant pairs Undirected.csv', header=0)\n",
    "data_dis = data_dis[data_dis[\"Hops\"]>11] # >11 to reduce time\n",
    "\n",
    "data_dis[\"Hops\"] = y_transform(data_dis[\"Hops\"])\n",
    "y_test_dis = pd.to_numeric(data_dis.Hops)#.astype(int)\n",
    "x_test_dis = np.vectorize(joining)(data_dis.Head.astype(str),data_dis.Tail.astype(str))\n",
    "\n",
    "distant_loader = DataLoader(tuple(zip(x_test_dis,y_test_dis)), batch_size=1, shuffle=True)\n",
    "distant_pairs_metrics = test_model(model_PMB, FE_inputs, distant_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.DataFrame([last_epoch, balanced_test_metrics, balanced_train_metrics, distant_pairs_metrics])\n",
    "# temp.to_csv('testout.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
